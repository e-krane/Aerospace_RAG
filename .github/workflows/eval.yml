name: RAG Pipeline Evaluation

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  workflow_dispatch:  # Allow manual trigger

env:
  PYTHON_VERSION: "3.11"

jobs:
  evaluate:
    name: Evaluate RAG Pipeline Quality
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install pytest pytest-cov

      - name: Load baseline metrics
        id: baseline
        run: |
          if [ -f "data/evaluation/baseline_metrics.json" ]; then
            echo "Baseline metrics found"
            cat data/evaluation/baseline_metrics.json
          else
            echo "No baseline metrics - will establish new baseline"
          fi

      - name: Run RAG pipeline tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          pytest tests/test_rag_pipeline.py \
            --verbose \
            --tb=short \
            --junit-xml=test-results/eval-results.xml \
            --cov=src \
            --cov-report=xml:coverage.xml \
            --cov-report=html:htmlcov \
            || echo "Tests failed - checking degradation threshold"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: test-results/

      - name: Upload coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            coverage.xml
            htmlcov/

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const junit = fs.readFileSync('test-results/eval-results.xml', 'utf8');

            // Parse junit results (simplified)
            const testCount = (junit.match(/<testcase/g) || []).length;
            const failCount = (junit.match(/<failure/g) || []).length;
            const passCount = testCount - failCount;

            const comment = `## 🤖 RAG Pipeline Evaluation Results

            **Tests:** ${passCount}/${testCount} passed

            ${failCount > 0 ? '⚠️ **Some tests failed - check details below**' : '✅ **All tests passed!**'}

            📊 [View detailed coverage report in artifacts]

            ---
            *Automated evaluation with DeepEval metrics*
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Check for metric degradation
        if: always()
        run: |
          # This will fail the build if metrics degrade >5%
          # The actual check is done in the test code
          # This step just ensures the workflow fails appropriately
          echo "Checking for metric degradation..."

          if [ -f "test-results/eval-results.xml" ]; then
            # Check if any tests failed due to degradation
            if grep -q "degraded" test-results/eval-results.xml; then
              echo "❌ Metrics degraded beyond acceptable threshold (>5%)"
              exit 1
            else
              echo "✅ No significant metric degradation detected"
            fi
          fi

  update-baseline:
    name: Update Baseline Metrics
    needs: evaluate
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/master'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test results
        uses: actions/download-artifact@v4
        with:
          name: test-results
          path: test-results/

      - name: Update baseline metrics
        run: |
          # Extract metrics from test results and update baseline
          # This would parse the test output and update baseline_metrics.json
          echo "Baseline update logic would go here"

          # For now, just show current metrics
          if [ -f "data/evaluation/baseline_metrics.json" ]; then
            echo "Current baseline:"
            cat data/evaluation/baseline_metrics.json
          fi

      - name: Commit updated baseline (if changed)
        run: |
          if [ -f "data/evaluation/baseline_metrics.json" ]; then
            git config --local user.email "action@github.com"
            git config --local user.name "GitHub Action"
            git add data/evaluation/baseline_metrics.json
            git diff --quiet && git diff --staged --quiet || git commit -m "chore: update baseline metrics [skip ci]"
            # Uncomment to auto-push baseline updates:
            # git push
          fi
